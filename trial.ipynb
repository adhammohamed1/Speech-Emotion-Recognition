{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install playsound\n",
    "#%pip install tensorflow --user\n",
    "#%pip install librosa\n",
    "#%pip install torch --user\n",
    "#%pip install optuna --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acceleration is on:  cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wave\n",
    "import librosa\n",
    "import os\n",
    "from IPython.display import Audio\n",
    "import IPython.display as ipd\n",
    "from playsound import playsound\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch, gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"The acceleration is on: \", device)\n",
    "stringLabels = [\"SAD\", \"ANG\", \"NEU\", \"HAP\", \"FEA\", \"DIS\"]\n",
    "labelToIndex = {stringLabels[i]: i for i in range(len(stringLabels))}\n",
    "files = np.array([])\n",
    "labels = np.array([])\n",
    "maxLen = 110361\n",
    "for dirname, _, filenames in os.walk('dataset'):\n",
    "    for filename in filenames:\n",
    "        # Reading the wav file\n",
    "        if filename.endswith('.wav'):\n",
    "            # Read the wav file (mono)\n",
    "            files = np.append(files, os.path.join(dirname, filename))\n",
    "            labels = np.append(labels, filename.split(\"_\")[2])\n",
    "def playAudio(path):\n",
    "    playsound(path)\n",
    "\n",
    "def getSamplingFrequency(path):\n",
    "    fileData = wave.open(path, 'rb')\n",
    "    # Extract Raw Audio from Wav File\n",
    "    signal = fileData.readframes(-1)\n",
    "    signal = np.frombuffer(signal, dtype='int16')\n",
    "    # Get the frame rate\n",
    "    frameRate = fileData.getframerate()\n",
    "    return frameRate\n",
    "\n",
    "def getAudioSignal(path):\n",
    "    fileData = wave.open(path, 'rb')\n",
    "    # Extract Raw Audio from Wav File\n",
    "    signal = fileData.readframes(-1)\n",
    "    signal = np.frombuffer(signal, dtype='int16')\n",
    "    # Get the frame rate\n",
    "    frameRate = fileData.getframerate()\n",
    "    # Find the time of the audio file\n",
    "    time = np.linspace(0, len(signal) / frameRate, num=len(signal))\n",
    "    return signal, time\n",
    "\n",
    "\n",
    "def calculateZeroCrossingRate(audioPath, frameSize, hopSize):\n",
    "    audio, sr = librosa.load(audioPath, sr=None)# we can make mono = True to convert it to 1 channel only\n",
    "    return np.array(librosa.feature.zero_crossing_rate( y= audio, frame_length= frameSize, hop_length= hopSize)[0]) # We can make center=True to get more accurate results\n",
    "\n",
    "def calculateEnergy(audioPath, frameSize, hopSize):\n",
    "    \"\"\"\n",
    "    audioPath: The path of the audio file\n",
    "    frameSize: The number of samples in each frame\n",
    "    hopSize: The number of samples between successive frames\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(audioPath, sr=None)# we can make mono = True to convert it to 1 channel only\n",
    "    return np.array(librosa.feature.rms(y = audio, frame_length= frameSize, hop_length= hopSize)[0] ** 2) # We can make center=True to get more accurate results\n",
    "\n",
    "def calculateMelSpectogram(audioPath, n_fft, hopSize, n_mels):\n",
    "    \"\"\"\"\n",
    "    n_fft: The number of points in the FFT, which determines the frequency resolution of the spectrogram\n",
    "    hopSize: The number of samples between successive frames\n",
    "    n_mels: The number of mel frequency bands to generate in the spectrogram or the number of filters in the filterbank\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(audioPath, sr=None)# we can make mono = True to convert it to 1 channel only\n",
    "    mel_spec = librosa.feature.melspectrogram(y= audio, sr=sr, n_fft=n_fft, hop_length=hopSize, n_mels=n_mels)\n",
    "    mel_spec_db  = librosa.power_to_db(mel_spec)\n",
    "    # Normalize between 0 and 1\n",
    "    if mel_spec_db.max() - mel_spec_db.min() != 0:\n",
    "        mel_spec_db = (mel_spec_db - mel_spec_db.min()) / ((mel_spec_db.max() - mel_spec_db.min()) + 1e-6) # eps for numerical stability\n",
    "    else:\n",
    "        mel_spec_db = mel_spec_db - mel_spec_db.min()\n",
    "    return mel_spec_db\n",
    "\n",
    "def addNoise(y, sr):\n",
    "    noise = np.random.randn(len(y))\n",
    "    y_noise = y + 0.005 * noise\n",
    "    return y_noise\n",
    "\n",
    "def changeSpeed(y, sr):\n",
    "    speed_change = np.random.uniform(low=0.9, high=1.1)\n",
    "    return librosa.effects.time_stretch(y, rate = speed_change)\n",
    "\n",
    "def timeShift(y, sr):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5) * 1000)\n",
    "    return np.roll(y, shift_range)\n",
    "\n",
    "def pitchShift(y, sr):\n",
    "    pitch_change = int(np.random.randint(low=-3, high = 3))\n",
    "    return librosa.effects.pitch_shift(y, sr = sr, n_steps=pitch_change)\n",
    "\n",
    "def volumeScale(y, sr):\n",
    "    volume_change = np.random.uniform(low=0.5, high = 1.5)\n",
    "    return y * volume_change\n",
    "\n",
    "def augmentData(data, label, savePath, noise, time_shift, change_speed, pitch_shift, volume_scale):\n",
    "    augmentedData = []\n",
    "    augmentedLabel = []\n",
    "    flag = 0\n",
    "    if not os.path.exists(savePath):\n",
    "        os.makedirs(savePath)\n",
    "        flag = 1\n",
    "        \n",
    "    for i in range(len(data)):\n",
    "        fileName = data[i][8:23]\n",
    "        \n",
    "        if not flag:\n",
    "            # Appending all of them to the augmentedData\n",
    "            augmentedData.append(data[i])\n",
    "            augmentedLabel.append(label[i])\n",
    "            if noise:\n",
    "                augmentedData.append(savePath+ \"/\" + str(fileName)+\"_noised.wav\")\n",
    "                augmentedLabel.append(label[i])\n",
    "            if change_speed:\n",
    "                augmentedData.append(savePath+ \"/\" + str(fileName)+\"_changeSpeed.wav\")\n",
    "                augmentedLabel.append(label[i])\n",
    "            if time_shift:\n",
    "                augmentedData.append(savePath+ \"/\" + str(fileName)+\"_timeShift.wav\")\n",
    "                augmentedLabel.append(label[i])\n",
    "            if pitch_shift:\n",
    "                augmentedData.append(savePath+ \"/\" + str(fileName)+\"_pitchShift.wav\")\n",
    "                augmentedLabel.append(label[i])\n",
    "            if volume_scale:\n",
    "                augmentedData.append(savePath+ \"/\" + str(fileName)+\"_volumeScale.wav\")\n",
    "                augmentedLabel.append(label[i])\n",
    "            continue\n",
    "        \n",
    "        augmentedData.append(data[i])\n",
    "        augmentedLabel.append(label[i])\n",
    "\n",
    "        audio, sr = librosa.load(data[i], sr=None)\n",
    "        if noise:\n",
    "            noisedAudio = addNoise(audio, sr)\n",
    "            if( maxLen > len(noisedAudio) ):\n",
    "                noisedAudio = np.pad(noisedAudio, (0, maxLen - len(noisedAudio)), 'constant')\n",
    "            noisedAudio = noisedAudio[:maxLen]\n",
    "            sf.write(savePath +  \"/\" + str(fileName)+\"_noised.wav\", noisedAudio, sr)\n",
    "            augmentedData.append(savePath +  \"/\" + str(fileName)+\"_noised.wav\")\n",
    "            augmentedLabel.append(label[i])\n",
    "        if change_speed:\n",
    "            changeSpeedAudio = changeSpeed(audio, sr)\n",
    "            if( maxLen > len(changeSpeedAudio) ):\n",
    "                changeSpeedAudio = np.pad(changeSpeedAudio, (0, maxLen - len(changeSpeedAudio)), 'constant')\n",
    "            changeSpeedAudio = changeSpeedAudio[:maxLen]\n",
    "            sf.write(savePath +  \"/\" + str(fileName)+\"_changeSpeed.wav\", changeSpeedAudio, sr)\n",
    "            augmentedData.append(savePath +  \"/\" + str(fileName)+\"_changeSpeed.wav\")\n",
    "            augmentedLabel.append(label[i])\n",
    "        if time_shift:\n",
    "            timeShiftAudio = timeShift(audio, sr)\n",
    "            if( maxLen > len(timeShiftAudio) ):\n",
    "                timeShiftAudio = np.pad(timeShiftAudio, (0, maxLen - len(timeShiftAudio)), 'constant')\n",
    "            timeShiftAudio = timeShiftAudio[:maxLen]\n",
    "            sf.write(savePath +  \"/\" + str(fileName)+\"_timeShift.wav\", timeShiftAudio, sr)\n",
    "            augmentedData.append(savePath +  \"/\" + str(fileName)+\"_timeShift.wav\")\n",
    "            augmentedLabel.append(label[i])\n",
    "        if pitch_shift:\n",
    "            pitchShiftAudio = pitchShift(audio, sr)\n",
    "            if( maxLen > len(pitchShiftAudio) ):\n",
    "                pitchShiftAudio = np.pad(pitchShiftAudio, (0, maxLen - len(pitchShiftAudio)), 'constant')\n",
    "            pitchShiftAudio = pitchShiftAudio[:maxLen]\n",
    "            sf.write(savePath +  \"/\" + str(fileName)+\"_pitchShift.wav\", pitchShiftAudio, sr)\n",
    "            augmentedData.append(savePath +  \"/\" + str(fileName)+\"_pitchShift.wav\")\n",
    "            augmentedLabel.append(label[i])\n",
    "        if volume_scale:\n",
    "            volumeScaleAudio = volumeScale(audio, sr)\n",
    "            if( maxLen > len(volumeScaleAudio) ):\n",
    "                volumeScaleAudio = np.pad(volumeScaleAudio, (0, maxLen - len(volumeScaleAudio)), 'constant')\n",
    "            volumeScaleAudio = volumeScaleAudio[:maxLen]\n",
    "            sf.write(savePath +  \"/\" + str(fileName)+\"_volumeScale.wav\", volumeScaleAudio, sr)\n",
    "            augmentedData.append(savePath +  \"/\" + str(fileName)+\"_volumeScale.wav\")\n",
    "            augmentedLabel.append(label[i])\n",
    "    return augmentedData, augmentedLabel\n",
    "\n",
    "def plotMelSpectogram(audioPath, n_fft, hopSize, n_mels):\n",
    "    audio, sr = librosa.load(audioPath, sr=None)# we can make mono = True to convert it to 1 channel only\n",
    "    mel_spec = librosa.feature.melspectrogram(y= audio, sr=sr, n_fft=n_fft, hop_length=hopSize, n_mels=n_mels)\n",
    "    mel_spec_db  = librosa.power_to_db(mel_spec)\n",
    "    librosa.display.specshow(mel_spec_db, sr=sr, hop_length=hopSize, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Log Mel spectrogram')\n",
    "    plt.show()\n",
    "\n",
    "def plotSpectrogram(audioPath, time):\n",
    "    audio, sr = librosa.load(audioPath, sr=None)# we can make mono = True to convert it to 1 channel only\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.specgram(audio, Fs=sr)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Spectrogram')\n",
    "    plt.show()\n",
    "    \n",
    "def plotSignal(signal, time):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(time, signal)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Audio Signal')\n",
    "    plt.show()\n",
    "\n",
    "def plotHistogram(data, dataTitle):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.hist(data, bins=15)\n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of the ' + dataTitle)\n",
    "    plt.show()\n",
    "\n",
    "def extractFeatures(data, labels, n_fft, hopSize, n_mels):\n",
    "    output = np.empty((len(data), 4), dtype=np.ndarray)\n",
    "    \n",
    "    for i, audioFile in enumerate(data):\n",
    "        # Calculate the zero crossing rate\n",
    "        zeroCrossingRate = calculateZeroCrossingRate(audioFile, n_fft, hopSize)\n",
    "        # Calculate the energy\n",
    "        energy = calculateEnergy(audioFile, n_fft, hopSize)\n",
    "        # Calculate the Mel Spectrogram\n",
    "        melSpectrogram = calculateMelSpectogram(audioFile, n_fft, hopSize, n_mels)\n",
    "        # Append the features to the output array\n",
    "        output[i][0] = zeroCrossingRate\n",
    "        output[i][1] = energy\n",
    "        output[i][2] = melSpectrogram\n",
    "        output[i][3] = labels[i]\n",
    "        output[i][0] = output[i][0].reshape(1, output[i][0].shape[0])\n",
    "        output[i][1] = output[i][1].reshape(1, output[i][1].shape[0])\n",
    "    return output\n",
    "\n",
    "def getFeatures(files, labels, n_fft, hop_size, n_mels, noise = False, time_shift = False, change_speed = False, pitch_shift = False, volume_scale = False):\n",
    "    savingDirectory = \"processed/n_fft_\" + str(n_fft) + \"_hop_size_\" + str(hop_size) + \"_n_mels_\" + str(n_mels)\n",
    "    if noise:\n",
    "        savingDirectory += \"_noise\"\n",
    "    if time_shift:\n",
    "        savingDirectory += \"_timeShift\"\n",
    "    if change_speed:\n",
    "        savingDirectory += \"_changeSpeed\"\n",
    "    if pitch_shift:\n",
    "        savingDirectory += \"_pitchShift\"\n",
    "    if volume_scale:\n",
    "        savingDirectory += \"_volumeScale\"\n",
    "    if not os.path.exists(savingDirectory):\n",
    "        \n",
    "        x_train_val, x_test, y_train_val, y_test = train_test_split(files, labels, train_size=0.7, random_state=42, stratify=labels)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, train_size=0.95, random_state=42, stratify=y_train_val)\n",
    "        \n",
    "        # Now let's augment the training data \n",
    "        augmentedData, augmentedLabel = augmentData(x_train, y_train, f\"augmented/n_fft_{n_fft}_hop_size_{hop_size}_n_mels_{n_mels}\", noise, time_shift, change_speed, pitch_shift, volume_scale)\n",
    "        x_train = augmentedData\n",
    "        y_train = augmentedLabel\n",
    "\n",
    "        # Extract the features\n",
    "        x_train = extractFeatures(x_train, y_train, n_fft, hop_size, n_mels)\n",
    "        x_val = extractFeatures(x_val, y_val, n_fft, hop_size, n_mels)\n",
    "        x_test = extractFeatures(x_test, y_test, n_fft, hop_size, n_mels)\n",
    "        \n",
    "        os.makedirs(savingDirectory)\n",
    "        np.save(savingDirectory + \"/x_train.npy\", x_train)\n",
    "        np.save(savingDirectory + \"/x_val.npy\", x_val)\n",
    "        np.save(savingDirectory + \"/x_test.npy\", x_test)\n",
    "        np.save(savingDirectory + \"/y_train.npy\", y_train)\n",
    "        np.save(savingDirectory + \"/y_val.npy\", y_val)\n",
    "        np.save(savingDirectory + \"/y_test.npy\", y_test)\n",
    "    else:\n",
    "        x_train = np.load(savingDirectory + \"/x_train.npy\", allow_pickle=True)\n",
    "        x_val = np.load(savingDirectory + \"/x_val.npy\", allow_pickle=True)\n",
    "        x_test = np.load(savingDirectory + \"/x_test.npy\", allow_pickle=True)\n",
    "        y_train = np.load(savingDirectory + \"/y_train.npy\", allow_pickle=True)\n",
    "        y_val = np.load(savingDirectory + \"/y_val.npy\", allow_pickle=True)\n",
    "        y_test = np.load(savingDirectory + \"/y_test.npy\", allow_pickle=True)\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.zcr = torch.tensor([data[i][0] for i in range(len(data))], dtype=torch.float32)\n",
    "        self.energy = torch.tensor([data[i][1] for i in range(len(data))], dtype=torch.float32)\n",
    "        self.melSpectogram = torch.tensor([data[i][2] for i in range(len(data))], dtype=torch.float32)\n",
    "        \n",
    "        indexLabels = [labelToIndex[labels[i]] for i in range(len(labels))]\n",
    "        oneHotLabels = np.zeros((len(labels), len(stringLabels)))\n",
    "        oneHotLabels[np.arange(len(labels)), indexLabels] = 1\n",
    "        self.labels = torch.tensor(oneHotLabels, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.zcr)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.zcr[idx], self.energy[idx], self.melSpectogram[idx], self.labels[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpoNet(nn.Module, hyperparams):\n",
    "    hyperparameters = None\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        super(ExpoNet, self).__init__()\n",
    "        self.hyperparameters = hyperparams\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.softmax(self.lastdenselayer, dim = 1)\n",
    "        return x\n",
    "    \n",
    "    def trainEpochs(self, criterion, optimizer, train_loader, val_loader, num_epochs = 10, lr_decay = False, lr_decay_epoch = 5, lr_decay_factor = 0.1):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            self.train()\n",
    "            for i, data in enumerate(train_loader):\n",
    "                _, _, melSpectogram, labels = data\n",
    "                melSpectogram, labels =  melSpectogram.to(device), labels.to(device)\n",
    "                # unsqueeze the melSpectogram to add a channel dimension\n",
    "                melSpectogram = melSpectogram.unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(melSpectogram)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * melSpectogram.size(0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                self.eval()\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    _, _, melSpectogram, labels = data\n",
    "                    melSpectogram, labels = melSpectogram.to(device), labels.to(device)\n",
    "                    melSpectogram = melSpectogram.unsqueeze(1)\n",
    "                    outputs = self(melSpectogram)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * melSpectogram.size(0)\n",
    "                \n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            val_loss = val_loss / len(val_loader.dataset)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            print(\"Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\".format(epoch+1, train_loss, val_loss))\n",
    "            if lr_decay and (epoch+1) % lr_decay_epoch == 0:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] *= lr_decay_factor\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        return torch.argmax(x, dim=1)\n",
    "    \n",
    "    def test(self, test_loader):\n",
    "        self.eval()\n",
    "        accuracy = 0\n",
    "        for i, data in enumerate(test_loader):\n",
    "            _, _, melSpectogram, labels = data\n",
    "            melSpectogram, labels = melSpectogram.to(device), labels.to(device)\n",
    "            melSpectogram = melSpectogram.unsqueeze(1)\n",
    "            output = self.predict(melSpectogram)\n",
    "            label = torch.argmax(labels, dim=1)\n",
    "            accuracy += torch.sum(output == label)\n",
    "        accuracy = accuracy / len(test_loader.dataset)\n",
    "        print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n",
    "\n",
    "    def analyze(self, test_loader):\n",
    "        self.eval()\n",
    "        output = []\n",
    "        label = []\n",
    "        for i, data in enumerate(test_loader):\n",
    "            _, _, melSpectogram, labels = data\n",
    "            melSpectogram, labels = melSpectogram.to(device), labels.to(device)\n",
    "            melSpectogram = melSpectogram.unsqueeze(1)\n",
    "            output.append(self.predict(melSpectogram))\n",
    "            label.append(torch.argmax(labels, dim=1))\n",
    "        output = torch.cat(output)\n",
    "        label = torch.cat(label)\n",
    "        print(classification_report(label.cpu(), output.cpu(), target_names=stringLabels))\n",
    "        ConfusionMatrixDisplay(confusion_matrix(label.cpu(), output.cpu()), display_labels=stringLabels).plot()\n",
    "        plt.show()\n",
    "\n",
    "    def updateShape(self, input_shape, kernel_size, stride, padding):\n",
    "        input_shape = (input_shape + 2 * padding - kernel_size) // stride + 1\n",
    "        return input_shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "N_FFT = 512\n",
    "HOP_SIZE = 160\n",
    "N_MELS = 40\n",
    "noise = True\n",
    "time_shift = True\n",
    "change_speed = True\n",
    "pitch_shift = True\n",
    "volume_scale = True\n",
    "\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = getFeatures(files=files, labels=labels, n_fft=N_FFT, hop_size = HOP_SIZE, n_mels= N_MELS, noise = noise, time_shift = time_shift, change_speed = change_speed, pitch_shift = pitch_shift, volume_scale = volume_scale)\n",
    "print(\"Extracted Features for the Data Successfully!\")\n",
    "print(\"Number of Training Examples: \", len(x_train))\n",
    "print(\"Number of Validation Examples: \", len(x_val))\n",
    "print(\"Number of Testing Examples: \", len(x_test))\n",
    "\n",
    "train_dataset = AudioDataset(x_train, y_train)\n",
    "val_dataset = AudioDataset(x_val, y_val)\n",
    "test_dataset = AudioDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.00001\n",
    "LR_DECAY = True\n",
    "LR_DECAY_EPOCH = 30\n",
    "LR_DECAY_FACTOR = 0.5\n",
    "WEIGHT_DECAY = 0.0001 # L2 regularization\n",
    "hyperparams = {'batch_size': BATCH_SIZE, 'epochs': EPOCHS, 'learning_rate': LEARNING_RATE, 'lr_decay': LR_DECAY, 'lr_decay_epoch': LR_DECAY_EPOCH, 'lr_decay_factor': LR_DECAY_FACTOR, 'weight_decay': WEIGHT_DECAY, 'nfft': N_FFT, 'hop_size': HOP_SIZE, 'n_mels': N_MELS, 'noise': noise, 'time_shift': time_shift, 'change_speed': change_speed, 'pitch_shift': pitch_shift, 'volume_scale': volume_scale, 'labels': 'all'}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(\"Created DataLoaders Successfully!\")\n",
    "\n",
    "model = ExpoNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay= WEIGHT_DECAY)\n",
    "model.trainEpochs(criterion, optimizer, train_dataloader, val_dataloader, num_epochs=EPOCHS, lr_decay=LR_DECAY, lr_decay_epoch=LR_DECAY_EPOCH, lr_decay_factor=LR_DECAY_FACTOR)\n",
    "model.analyze(train_dataloader)\n",
    "model.analyze(val_dataloader)\n",
    "model.test(test_dataloader)\n",
    "\n",
    "modelParams = f\"models/ExpoNet_BATCHSIZE{BATCH_SIZE}_LR_{LEARNING_RATE}_Nfft{N_FFT}_hopsize_{HOP_SIZE}_nmels_{N_MELS}_epochs_{EPOCHS}_lr_decay_{LR_DECAY}_lr_decay_epoch_{LR_DECAY_EPOCH}_lr_decay_factor_{LR_DECAY_FACTOR}_weight_decay_{WEIGHT_DECAY}.pth\"\n",
    "torch.save(model.state_dict(), modelParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.analyze(test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet Variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetVariant(nn.Module):\n",
    "\n",
    "    hyperparameters = None\n",
    "    input_shape = None\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    current_epoch = 0\n",
    "\n",
    "    def __init__(self, input_shape, hyperparameters = None):\n",
    "        super(AlexNetVariant, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=(5, 5))\n",
    "        self.updateShape(5, 1, 0)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "        self.updateShape(3, 3, 0)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=(5, 5))\n",
    "        self.updateShape(5, 1, 0)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "        self.updateShape(3, 3, 0)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=(3, 3))\n",
    "        self.updateShape(3, 1, 0)\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=(3, 3))\n",
    "        self.updateShape(3, 1, 0)\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(p=0.4)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=(3, 3))\n",
    "        self.updateShape(3, 1, 0)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "        self.updateShape(3, 3, 0)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        flattened = self.input_shape[0] * self.input_shape[1] * 256\n",
    "        self.fc1 = nn.Linear(in_features=flattened, out_features=4096)\n",
    "        self.fc2 = nn.Linear(in_features=4096, out_features=4096)\n",
    "        self.fc3 = nn.Linear(in_features=4096, out_features=6)\n",
    "    \n",
    "\n",
    "    def forward(self, x) :\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.maxpool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.maxpool2(x)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.maxpool3(x)  \n",
    "        x = self.bn1(x)\n",
    "        x = flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(self.fc3(x), dim=1) # Log softmax instead of softmax for numerical stability and faster computation\n",
    "\n",
    "    def trainEpochs(self, criterion, optimizer, train_loader, val_loader, num_epochs = 10, lr_decay = False, lr_decay_epoch = 5, lr_decay_factor = 0.1, save_each = None):\n",
    "        num_epochs = self.hyperparameters['epochs']\n",
    "        for epoch in range(self.current_epoch, num_epochs):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            self.train()\n",
    "            for i, data in enumerate(train_loader):\n",
    "                _, _, melSpectogram, labels = data\n",
    "                melSpectogram, labels =  melSpectogram.to(device), labels.to(device)\n",
    "                melSpectogram = melSpectogram.unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(melSpectogram)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * melSpectogram.size(0)\n",
    "\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    _, _, melSpectogram, labels = data\n",
    "                    melSpectogram, labels = melSpectogram.to(device), labels.to(device)\n",
    "                    melSpectogram = melSpectogram.unsqueeze(1)\n",
    "                    outputs = self(melSpectogram)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * melSpectogram.size(0)\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            val_loss = val_loss / len(val_loader.dataset)\n",
    "            self.training_losses.append(train_loss)\n",
    "            self.validation_losses.append(val_loss)\n",
    "\n",
    "            print(\"Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\".format(self.current_epoch+1, train_loss, val_loss))\n",
    "            if lr_decay and (epoch+1) % lr_decay_epoch == 0:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] *= lr_decay_factor\n",
    "            if save_each and (epoch+1) % save_each == 0:\n",
    "                self.saveModel()\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        return torch.argmax(x, dim=1)\n",
    "    \n",
    "    def analyze(self, test_loader):\n",
    "        self.eval()\n",
    "        output = []\n",
    "        label = []\n",
    "        for i, data in enumerate(test_loader):\n",
    "            _, _, melSpectogram, labels = data\n",
    "            melSpectogram, labels = melSpectogram.to(device), labels.to(device)\n",
    "            melSpectogram = melSpectogram.unsqueeze(1)\n",
    "            output.append(self.predict(melSpectogram))\n",
    "            label.append(torch.argmax(labels, dim=1))\n",
    "        output = torch.cat(output)\n",
    "        label = torch.cat(label)\n",
    "        self.plotHistory()\n",
    "        print(classification_report(label.cpu(), output.cpu(), target_names=stringLabels))\n",
    "        ConfusionMatrixDisplay(confusion_matrix(label.cpu(), output.cpu()), display_labels=stringLabels).plot()\n",
    "        plt.show()\n",
    "\n",
    "    def updateShape(self, kernel_size, stride, padding):\n",
    "        self.input_shape[0] = (self.input_shape[0] - kernel_size + 2*padding) // stride + 1\n",
    "        self.input_shape[1] = (self.input_shape[1] - kernel_size + 2*padding) // stride + 1\n",
    "  \n",
    "    def plotHistory(self):\n",
    "        plt.plot(self.train_losses, label = \"Train\")\n",
    "        plt.plot(self.validation_losses, label = \"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def saveModel(self):\n",
    "        path = \"models/AlexNetVariant_\"\n",
    "        for key, value in self.hyperparameters.items():\n",
    "            path += key + \"_\" + str(value) + \"_\"\n",
    "        path += \"current_epoch_\" + str(self.current_epoch +1)\n",
    "        path += \".pt\"\n",
    "        torch.save(self.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yousef\\AppData\\Local\\Temp\\ipykernel_15916\\645635343.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  self.zcr = torch.tensor([data[i][0] for i in range(len(data))], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "N_FFT = 512\n",
    "HOP_SIZE = 160\n",
    "N_MELS = 40\n",
    "noise = False\n",
    "time_shift = False\n",
    "change_speed = False\n",
    "pitch_shift = False\n",
    "volume_scale = False\n",
    "\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = getFeatures(files=files, labels=labels, n_fft=N_FFT, hop_size = HOP_SIZE, n_mels= N_MELS, noise = noise, time_shift = time_shift, change_speed = change_speed, pitch_shift = pitch_shift, volume_scale = volume_scale)\n",
    "\n",
    "train_dataset = AudioDataset(x_train, y_train)\n",
    "val_dataset = AudioDataset(x_val, y_val)\n",
    "test_dataset = AudioDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-5\n",
    "LR_DECAY = True\n",
    "LR_DECAY_EPOCH = 30\n",
    "LR_DECAY_FACTOR = 0.25\n",
    "WEIGHT_DECAY = 1e-2\n",
    "SAVE_EACH = 5\n",
    "\n",
    "hyperparameters = {\"batch_size\": BATCH_SIZE, \"epochs\": EPOCHS, \"learning_rate\": LEARNING_RATE, \"lr_decay\": LR_DECAY, \"lr_decay_epoch\": LR_DECAY_EPOCH, \"lr_decay_factor\": LR_DECAY_FACTOR, \"weight_decay\": WEIGHT_DECAY}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = AlexNetVariant([x_train[0][2].shape[0], x_train[0][2].shape[1]], hyperparameters = hyperparameters).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay= WEIGHT_DECAY)\n",
    "model.trainEpochs(criterion, optimizer, test_dataloader, val_dataloader, lr_decay=LR_DECAY, lr_decay_epoch=LR_DECAY_EPOCH, lr_decay_factor=LR_DECAY_FACTOR, save_each = SAVE_EACH)\n",
    "model.analyze(test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
